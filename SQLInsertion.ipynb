{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSQL Crendentials and Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection parameters\n",
    "db_config = {\n",
    "    'host': 'database-1.cb0gsmcyc0ns.us-west-1.rds.amazonaws.com',\n",
    "    'port': 5432,\n",
    "    'dbname': 'fitbit_data',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Pass1234!'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    "engine = create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to the default 'postgres' database to create a new one\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Pass1234!\",\n",
    "    host=\"database-1.cb0gsmcyc0ns.us-west-1.rds.amazonaws.com\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True  # Required to run CREATE DATABASE outside a transaction\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a new database\n",
    "db_name = \"fitbit_data\"\n",
    "cur.execute(f\"CREATE DATABASE {db_name};\")\n",
    "\n",
    "print(f\"Database '{db_name}' created successfully!\")\n",
    "\n",
    "# Close the connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating MET and metric_explaination table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "# Execute SQL using text()\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS metric_explaination (\n",
    "        metric_id SERIAL PRIMARY KEY,\n",
    "        metric_name VARCHAR(100) NOT NULL,\n",
    "        min_value FLOAT NOT NULL,\n",
    "        max_value FLOAT NOT NULL,\n",
    "        min_anamoly_desc VARCHAR(100) NOT NULL,\n",
    "        max_anamoly_desc VARCHAR(100) NOT NULL\n",
    "    );\n",
    "    \"\"\"))\n",
    "    print(\"Table created successfully!\")\n",
    "    \n",
    "    connection.commit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Execute SQL using text()\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS met_data (\n",
    "        met_min FLOAT NOT NULL,\n",
    "        met_max FLOAT NOT NULL,\n",
    "        likely_activity VARCHAR(100) NOT NULL\n",
    "    );\n",
    "    \"\"\"))\n",
    "    print(\"Table created successfully!\")\n",
    "    \n",
    "    connection.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"Metric Explanations - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.columns = ['metric_id', 'metric_name', 'min_value', 'max_value', 'min_anamoly_desc', 'max_anamoly_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.to_sql('metric_explaination', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met = pd.read_csv(\"Likely Activity - Sheet1.csv\")\n",
    "met.columns = ['met_min', 'met_max', 'likely_activity']\n",
    "met.to_sql('met_data', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating User Demographics and Daily Data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Execute SQL using text()\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_demographics (\n",
    "        user_id         BIGINT PRIMARY KEY,   \n",
    "        weight_kg       NUMERIC,\n",
    "        weight_pounds   NUMERIC,\n",
    "        bmi             NUMERIC,\n",
    "        age             INT,\n",
    "        smoker          BOOLEAN,\n",
    "        drinker         BOOLEAN,\n",
    "        updated_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\"))\n",
    "    print(\"Table created successfully!\")\n",
    "    \n",
    "    connection.commit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS daily_data (\n",
    "            user_id             BIGINT NOT NULL,\n",
    "            activity_date       DATE NOT NULL,\n",
    "            total_steps         INTEGER,\n",
    "            total_distance      NUMERIC,\n",
    "            total_calories      INTEGER,\n",
    "            total_sleep_minutes INTEGER,\n",
    "            very_active_minutes INTEGER,\n",
    "            fairly_active_minutes INTEGER,\n",
    "            lightly_active_minutes INTEGER,\n",
    "            sedentary_minutes   INTEGER,\n",
    "            updated_at          TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            PRIMARY KEY (user_id, activity_date),\n",
    "            FOREIGN KEY (user_id) REFERENCES user_demographics(user_id)\n",
    "        );\n",
    "    \"\"\"))\n",
    "    print(\"Table created successfully!\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chira\\AppData\\Local\\Temp\\ipykernel_18392\\2857203167.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  combined_weight['Date'] = pd.to_datetime(combined_weight['Date'])\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def process_and_insert_user_data():\n",
    "    # Load weight data from 3 CSVs and combine them\n",
    "    dataset_paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/weightLogInfo_merged.csv',\n",
    "                     'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/weightLogInfo_merged.csv',\n",
    "                     'synthetic_data/synthetic_data/synthetic_weightLog.csv']\n",
    "    weight_dfs = []\n",
    "    for paths in dataset_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(paths)  # Adjust filenames as needed\n",
    "            weight_dfs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {paths} not found. Skipping.\")\n",
    "    \n",
    "    # Combine all weight dataframes\n",
    "    if weight_dfs:\n",
    "        combined_weight = pd.concat(weight_dfs, ignore_index=True)\n",
    "        \n",
    "        # Convert date to datetime and ensure ID is treated as int\n",
    "        combined_weight['Date'] = pd.to_datetime(combined_weight['Date'])\n",
    "        combined_weight['Id'] = combined_weight['Id'].astype(int)\n",
    "        \n",
    "        # Get the latest weight data for each user\n",
    "        latest_weight = combined_weight.sort_values('Date').groupby('Id').last().reset_index()\n",
    "        \n",
    "        # Rename columns to match our database schema\n",
    "        latest_weight = latest_weight.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'WeightKg': 'weight_kg',\n",
    "            'WeightPounds': 'weight_pounds',\n",
    "            'BMI': 'bmi'\n",
    "        })\n",
    "    else:\n",
    "        print(\"No weight data found.\")\n",
    "        latest_weight = pd.DataFrame(columns=['user_id', 'weight_kg', 'weight_pounds', 'bmi'])\n",
    "    \n",
    "    # Load demographics data\n",
    "    try:\n",
    "        demographics = pd.read_csv('C:/Users/Chira/OneDrive/Desktop/DSC 202/synthetic_demographic_data.csv')  # Adjust filename as needed\n",
    "        demographics['Id'] = demographics['Id'].astype(int)\n",
    "        \n",
    "        # Rename columns to match our schema\n",
    "        demographics = demographics.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'Age': 'age', \n",
    "            'Smoking': 'smoker',\n",
    "            'Drinking': 'drinker'\n",
    "        })\n",
    "        \n",
    "        # # Convert smoking and drinking to boolean\n",
    "        demographics['smoker'] = demographics['smoker'].map({1: True, 0: False})\n",
    "        demographics['drinker'] = demographics['drinker'].map({1: True, 0: False})\n",
    "    except FileNotFoundError:\n",
    "        print(\"Demographics file not found.\")\n",
    "        demographics = pd.DataFrame(columns=['user_id', 'age', 'smoker', 'drinker'])\n",
    "    \n",
    "    # Merge weight and demographics data on user_id\n",
    "    if not latest_weight.empty and not demographics.empty:\n",
    "        # Outer join to include all users from both datasets\n",
    "        merged_data = pd.merge(\n",
    "            latest_weight[['user_id', 'weight_kg', 'weight_pounds', 'bmi']], \n",
    "            demographics[['user_id', 'age', 'smoker', 'drinker']], \n",
    "            on='user_id',\n",
    "            how='right'\n",
    "        )\n",
    "    elif not latest_weight.empty:\n",
    "        merged_data = latest_weight[['user_id', 'weight_kg', 'weight_pounds', 'bmi']]\n",
    "        merged_data['age'] = np.nan\n",
    "        merged_data['smoker'] = None\n",
    "        merged_data['drinker'] = None\n",
    "    elif not demographics.empty:\n",
    "        merged_data = demographics[['user_id', 'age', 'smoker', 'drinker']]\n",
    "        merged_data['weight_kg'] = np.nan\n",
    "        merged_data['weight_pounds'] = np.nan\n",
    "        merged_data['bmi'] = np.nan\n",
    "    else:\n",
    "        print(\"No data available to insert.\")\n",
    "        return\n",
    "    \n",
    "    # Add updated_at column\n",
    "    merged_data['updated_at'] = datetime.now()\n",
    "    \n",
    "    return merged_data, latest_weight, demographics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_data,latest,demo = process_and_insert_user_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 70 records into user_demographics table...\n",
      "Data insertion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        print(f\"Inserting {len(merged_data)} records into user_demographics table...\")\n",
    "        merged_data.to_sql('user_demographics', engine, if_exists='append', index=False)\n",
    "        print(\"Data insertion completed successfully.\")\n",
    "except Exception as e:\n",
    "        print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('Merged_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id ActivityDate  TotalSteps  TotalDistance  TrackerDistance  \\\n",
      "0  1503960366    3/25/2016       11004           7.11             7.11   \n",
      "1  1503960366    3/26/2016       17609          11.55            11.55   \n",
      "2  1503960366    3/27/2016       12736           8.53             8.53   \n",
      "3  1503960366    3/28/2016       13231           8.93             8.93   \n",
      "4  1503960366    3/29/2016       12041           7.85             7.85   \n",
      "\n",
      "   LoggedActivitiesDistance  VeryActiveDistance  ModeratelyActiveDistance  \\\n",
      "0                       0.0                2.57                      0.46   \n",
      "1                       0.0                6.92                      0.73   \n",
      "2                       0.0                4.66                      0.16   \n",
      "3                       0.0                3.19                      0.79   \n",
      "4                       0.0                2.16                      1.09   \n",
      "\n",
      "   LightActiveDistance  SedentaryActiveDistance  VeryActiveMinutes  \\\n",
      "0                 4.07                      0.0                 33   \n",
      "1                 3.91                      0.0                 89   \n",
      "2                 3.71                      0.0                 56   \n",
      "3                 4.95                      0.0                 39   \n",
      "4                 4.61                      0.0                 28   \n",
      "\n",
      "   FairlyActiveMinutes  LightlyActiveMinutes  SedentaryMinutes  Calories  \n",
      "0                   12                   205               804      1819  \n",
      "1                   17                   274               588      2154  \n",
      "2                    5                   268               605      1944  \n",
      "3                   20                   224              1080      1932  \n",
      "4                   28                   243               763      1886  \n",
      "Loaded 457 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/dailyActivity_merged.csv\n",
      "           Id ActivityDate  TotalSteps  TotalDistance  TrackerDistance  \\\n",
      "0  1503960366    4/12/2016       13162           8.50             8.50   \n",
      "1  1503960366    4/13/2016       10735           6.97             6.97   \n",
      "2  1503960366    4/14/2016       10460           6.74             6.74   \n",
      "3  1503960366    4/15/2016        9762           6.28             6.28   \n",
      "4  1503960366    4/16/2016       12669           8.16             8.16   \n",
      "\n",
      "   LoggedActivitiesDistance  VeryActiveDistance  ModeratelyActiveDistance  \\\n",
      "0                       0.0                1.88                      0.55   \n",
      "1                       0.0                1.57                      0.69   \n",
      "2                       0.0                2.44                      0.40   \n",
      "3                       0.0                2.14                      1.26   \n",
      "4                       0.0                2.71                      0.41   \n",
      "\n",
      "   LightActiveDistance  SedentaryActiveDistance  VeryActiveMinutes  \\\n",
      "0                 6.06                      0.0                 25   \n",
      "1                 4.71                      0.0                 21   \n",
      "2                 3.91                      0.0                 30   \n",
      "3                 2.83                      0.0                 29   \n",
      "4                 5.04                      0.0                 36   \n",
      "\n",
      "   FairlyActiveMinutes  LightlyActiveMinutes  SedentaryMinutes  Calories  \n",
      "0                   13                   328               728      1985  \n",
      "1                   19                   217               776      1797  \n",
      "2                   11                   181              1218      1776  \n",
      "3                   34                   209               726      1745  \n",
      "4                   10                   221               773      1863  \n",
      "Loaded 940 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv\n",
      "  ActivityDate        Id  TotalSteps  TotalDistance  Calories  \\\n",
      "0    3/25/2016  99990001        2747       1.727786      1781   \n",
      "1    3/26/2016  99990001        7731       4.076389      2202   \n",
      "2    3/27/2016  99990001        6764       9.320733      2199   \n",
      "3    3/28/2016  99990001        7384       8.623866      2048   \n",
      "4    3/29/2016  99990001       10909       5.021166      2251   \n",
      "\n",
      "   VeryActiveMinutes  LightlyActiveMinutes  SedentaryMinutes  \n",
      "0                  4                   187              1058  \n",
      "1                 41                   159              1066  \n",
      "2                 12                   162              1165  \n",
      "3                  9                   163              1203  \n",
      "4                  8                   216              1051  \n",
      "Loaded 457 rows from synthetic_data/synthetic_data/synthetic_dailyActivity.csv\n",
      "      user_id activity_date  total_steps  total_distance  total_calories  \\\n",
      "0  1503960366    2016-03-25        11004            7.11            1819   \n",
      "1  1503960366    2016-03-26        17609           11.55            2154   \n",
      "2  1503960366    2016-03-27        12736            8.53            1944   \n",
      "3  1503960366    2016-03-28        13231            8.93            1932   \n",
      "4  1503960366    2016-03-29        12041            7.85            1886   \n",
      "\n",
      "   very_active_minutes  lightly_active_minutes  sedentary_minutes  \n",
      "0                   33                     205                804  \n",
      "1                   89                     274                588  \n",
      "2                   56                     268                605  \n",
      "3                   39                     224               1080  \n",
      "4                   28                     243                763  \n",
      "Loaded 198559 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv\n",
      "Loaded 188521 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv\n",
      "Loaded 198559 rows from synthetic_data/synthetic_data/synthetic_minuteSleep.csv\n",
      "Found potential duplicate records, resolving...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def process_and_insert_daily_data():\n",
    "    # Load and combine activity data from 3 CSVs\n",
    "    paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/dailyActivity_merged.csv', \n",
    "             'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv',\n",
    "             'synthetic_data/synthetic_data/synthetic_dailyActivity.csv']\n",
    "    activity_dfs = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            activity_dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {path} not found. Skipping.\")\n",
    "    \n",
    "    if activity_dfs:\n",
    "        # Combine all activity dataframes\n",
    "        combined_activity = pd.concat(activity_dfs, ignore_index=True)\n",
    "        \n",
    "        # Convert date column and ensure ID is treated as int\n",
    "        combined_activity['ActivityDate'] = pd.to_datetime(combined_activity['ActivityDate']).dt.date\n",
    "        combined_activity['Id'] = combined_activity['Id'].astype(int)\n",
    "        \n",
    "        # Rename columns to match our database schema\n",
    "        activity_data = combined_activity.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'ActivityDate': 'activity_date',\n",
    "            'TotalSteps': 'total_steps',\n",
    "            'TotalDistance': 'total_distance',\n",
    "            'Calories': 'total_calories',\n",
    "            'VeryActiveMinutes': 'very_active_minutes',\n",
    "            'LightlyActiveMinutes': 'lightly_active_minutes',\n",
    "            'SedentaryMinutes': 'sedentary_minutes'\n",
    "        })\n",
    "        \n",
    "        # Select only the columns we need\n",
    "        activity_data = activity_data[[\n",
    "            'user_id', 'activity_date', 'total_steps', 'total_distance', \n",
    "            'total_calories', 'very_active_minutes', 'lightly_active_minutes', \n",
    "            'sedentary_minutes'\n",
    "        ]]\n",
    "    else:\n",
    "        print(\"No activity data found.\")\n",
    "        activity_data = pd.DataFrame(columns=[\n",
    "            'user_id', 'activity_date', 'total_steps', 'total_distance', \n",
    "            'total_calories', 'very_active_minutes', 'lightly_active_minutes', \n",
    "            'sedentary_minutes'\n",
    "        ])\n",
    "    \n",
    "    # Load and process sleep data from 3 CSVs\n",
    "    sleep_dfs = []\n",
    "    paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv',\n",
    "                 'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv',\n",
    "                 'synthetic_data/synthetic_data/synthetic_minuteSleep.csv']\n",
    "    for path in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            sleep_dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {path} not found. Skipping.\")\n",
    "    \n",
    "    if sleep_dfs:\n",
    "        # Combine all sleep dataframes\n",
    "        combined_sleep = pd.concat(sleep_dfs, ignore_index=True)\n",
    "        \n",
    "        # Convert date column and ensure ID is treated as int\n",
    "        combined_sleep['date'] = pd.to_datetime(combined_sleep['date']).dt.date\n",
    "        combined_sleep['Id'] = combined_sleep['Id'].astype(int)\n",
    "        \n",
    "        # Sum up sleep minutes by user and date\n",
    "        sleep_data = combined_sleep.groupby(['Id', 'date'])['value'].sum().reset_index()\n",
    "        \n",
    "        # Rename columns to match our schema\n",
    "        sleep_data = sleep_data.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'date': 'activity_date',\n",
    "            'value': 'total_sleep_minutes'\n",
    "        })\n",
    "    else:\n",
    "        print(\"No sleep data found.\")\n",
    "        sleep_data = pd.DataFrame(columns=['user_id', 'activity_date', 'total_sleep_minutes'])\n",
    "    \n",
    "    # Merge activity and sleep data\n",
    "    if not activity_data.empty and not sleep_data.empty:\n",
    "        # Outer join to include all records from both datasets\n",
    "        merged_data = pd.merge(\n",
    "            activity_data, \n",
    "            sleep_data, \n",
    "            on=['user_id', 'activity_date'],\n",
    "            how='outer'\n",
    "        )\n",
    "    elif not activity_data.empty:\n",
    "        merged_data = activity_data\n",
    "        merged_data['total_sleep_minutes'] = None\n",
    "    elif not sleep_data.empty:\n",
    "        merged_data = sleep_data\n",
    "        for col in ['total_steps', 'total_distance', 'total_calories', \n",
    "                    'very_active_minutes', 'lightly_active_minutes', 'sedentary_minutes']:\n",
    "            merged_data[col] = None\n",
    "    else:\n",
    "        print(\"No data available to insert.\")\n",
    "        return\n",
    "    \n",
    "    # Add updated_at column\n",
    "    merged_data['updated_at'] = datetime.now()\n",
    "    \n",
    "    # Handle potential duplicates by using the most complete record\n",
    "    # Group by primary key and keep the record with the most non-null values\n",
    "    if len(merged_data) > merged_data.drop_duplicates(subset=['user_id', 'activity_date']).shape[0]:\n",
    "        print(\"Found potential duplicate records, resolving...\")\n",
    "        \n",
    "        # Count non-null values for each record\n",
    "        merged_data['non_null_count'] = merged_data.notnull().sum(axis=1)\n",
    "        \n",
    "        # Sort by count (descending) and take the first record for each user_id/date combination\n",
    "        merged_data = merged_data.sort_values(['user_id', 'activity_date', 'non_null_count'], \n",
    "                                             ascending=[True, True, False])\n",
    "        merged_data = merged_data.drop_duplicates(subset=['user_id', 'activity_date'])\n",
    "        merged_data = merged_data.drop(columns=['non_null_count'])\n",
    "    \n",
    "    return merged_data\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_data = process_and_insert_daily_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 2341 records into daily_data table...\n",
      "Data insertion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Insert data into PostgreSQL table\n",
    "try:\n",
    "    print(f\"Inserting {len(merged_data)} records into daily_data table...\")\n",
    "    \n",
    "    # Use on_conflict=\"do_nothing\" to handle potential primary key violations\n",
    "    merged_data.to_sql('daily_data', engine, if_exists='append', index=False,\n",
    "                        method='multi', chunksize=1000)\n",
    "    \n",
    "    print(\"Data insertion completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('daily_merged_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating hourly-data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "    CREATE TABLE hourly_data (\n",
    "        user_id         BIGINT NOT NULL,\n",
    "        activity_hour   TIMESTAMP NOT NULL,\n",
    "        steps           INTEGER,\n",
    "        calories        INTEGER,\n",
    "        sleep_minutes   INTEGER,\n",
    "        intensity_level INTEGER,  \n",
    "        updated_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        PRIMARY KEY (user_id, activity_hour),\n",
    "        FOREIGN KEY (user_id) REFERENCES user_demographics(user_id)\n",
    "        );\n",
    "    \"\"\"))\n",
    "    print(\"Table created successfully!\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24084 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlySteps_merged.csv\n",
      "Loaded 22099 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlySteps_merged.csv\n",
      "Loaded 24084 rows from synthetic_data/synthetic_data/synthetic_hourlySteps.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chira\\AppData\\Local\\Temp\\ipykernel_18392\\3281956432.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  combined_steps['ActivityHour'] = pd.to_datetime(combined_steps['ActivityHour'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24084 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlyCalories_merged.csv\n",
      "Loaded 22099 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlyCalories_merged.csv\n",
      "Loaded 24084 rows from synthetic_data/synthetic_data/synthetic_hourlyCalories.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chira\\AppData\\Local\\Temp\\ipykernel_18392\\3281956432.py:43: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  combined_calories['ActivityHour'] = pd.to_datetime(combined_calories['ActivityHour'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24084 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlyIntensities_merged.csv\n",
      "Loaded 22099 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlyIntensities_merged.csv\n",
      "Loaded 24084 rows from synthetic_data/synthetic_data/synthetic_hourlyIntensities.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chira\\AppData\\Local\\Temp\\ipykernel_18392\\3281956432.py:70: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  combined_intensity['ActivityHour'] = pd.to_datetime(combined_intensity['ActivityHour'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 198559 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv\n",
      "Loaded 188521 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv\n",
      "Loaded 198559 rows from synthetic_data/synthetic_data/synthetic_minuteSleep.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chira\\AppData\\Local\\Temp\\ipykernel_18392\\3281956432.py:102: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  combined_sleep['hour'] = combined_sleep['date'].dt.floor('H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found potential duplicate records, resolving...\n"
     ]
    }
   ],
   "source": [
    "def process_and_insert_hourly_data():\n",
    "    # Process hourly steps data\n",
    "    steps_dfs = []\n",
    "    paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlySteps_merged.csv',\n",
    "                 'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlySteps_merged.csv',\n",
    "                 'synthetic_data/synthetic_data/synthetic_hourlySteps.csv']\n",
    "    for file_path in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            steps_dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found. Skipping.\")\n",
    "    \n",
    "    if steps_dfs:\n",
    "        combined_steps = pd.concat(steps_dfs, ignore_index=True)\n",
    "        combined_steps['ActivityHour'] = pd.to_datetime(combined_steps['ActivityHour'])\n",
    "        combined_steps['Id'] = combined_steps['Id'].astype(int)\n",
    "        steps_data = combined_steps.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'ActivityHour': 'activity_hour',\n",
    "            'StepTotal': 'steps'\n",
    "        })\n",
    "    else:\n",
    "        print(\"No steps data found.\")\n",
    "        steps_data = pd.DataFrame(columns=['user_id', 'activity_hour', 'steps'])\n",
    "    \n",
    "    # Process hourly calories data\n",
    "    calories_dfs = []\n",
    "    paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlyCalories_merged.csv',\n",
    "                 'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlyCalories_merged.csv',\n",
    "                 'synthetic_data/synthetic_data/synthetic_hourlyCalories.csv']\n",
    "    for file_path in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            calories_dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found. Skipping.\")\n",
    "    \n",
    "    if calories_dfs:\n",
    "        combined_calories = pd.concat(calories_dfs, ignore_index=True)\n",
    "        combined_calories['ActivityHour'] = pd.to_datetime(combined_calories['ActivityHour'])\n",
    "        combined_calories['Id'] = combined_calories['Id'].astype(int)\n",
    "        calories_data = combined_calories.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'ActivityHour': 'activity_hour',\n",
    "            'Calories': 'calories'\n",
    "        })\n",
    "    else:\n",
    "        print(\"No calories data found.\")\n",
    "        calories_data = pd.DataFrame(columns=['user_id', 'activity_hour', 'calories'])\n",
    "    \n",
    "    # Process hourly intensity data\n",
    "    intensity_dfs = []\n",
    "    paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlyIntensities_merged.csv',\n",
    "                 'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlyIntensities_merged.csv',\n",
    "                 'synthetic_data/synthetic_data/synthetic_hourlyIntensities.csv']\n",
    "    for file_path in paths:\n",
    "        try:\n",
    "            # file_path = f'hourlyIntensity_{i}.csv'\n",
    "            df = pd.read_csv(file_path)\n",
    "            intensity_dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found. Skipping.\")\n",
    "    \n",
    "    if intensity_dfs:\n",
    "        combined_intensity = pd.concat(intensity_dfs, ignore_index=True)\n",
    "        combined_intensity['ActivityHour'] = pd.to_datetime(combined_intensity['ActivityHour'])\n",
    "        combined_intensity['Id'] = combined_intensity['Id'].astype(int)\n",
    "        # Use TotalIntensity as the intensity_level (you can change to AverageIntensity if preferred)\n",
    "        intensity_data = combined_intensity.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'ActivityHour': 'activity_hour',\n",
    "            'TotalIntensity': 'intensity_level'\n",
    "        })[['user_id', 'activity_hour', 'intensity_level']]\n",
    "    else:\n",
    "        print(\"No intensity data found.\")\n",
    "        intensity_data = pd.DataFrame(columns=['user_id', 'activity_hour', 'intensity_level'])\n",
    "        \n",
    "    # Process sleep data\n",
    "    sleep_dfs = []\n",
    "    paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv',\n",
    "                 'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv',\n",
    "                 'synthetic_data/synthetic_data/synthetic_minuteSleep.csv']\n",
    "    for file_path in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            sleep_dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found. Skipping.\")\n",
    "    \n",
    "    if sleep_dfs:\n",
    "        combined_sleep = pd.concat(sleep_dfs, ignore_index=True)\n",
    "        # Convert date and ensure ID is treated as int\n",
    "        combined_sleep['date'] = pd.to_datetime(combined_sleep['date'])\n",
    "        combined_sleep['Id'] = combined_sleep['Id'].astype(int)\n",
    "        \n",
    "        # Extract hour from datetime and group by user, date and hour\n",
    "        combined_sleep['hour'] = combined_sleep['date'].dt.floor('H')\n",
    "        \n",
    "        # Sum up sleep minutes by user and hour\n",
    "        sleep_data = combined_sleep.groupby(['Id', 'hour'])['value'].sum().reset_index()\n",
    "        \n",
    "        # Rename columns to match our schema\n",
    "        sleep_data = sleep_data.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'hour': 'activity_hour',\n",
    "            'value': 'sleep_minutes'\n",
    "        })\n",
    "    else:\n",
    "        print(\"No sleep data found.\")\n",
    "        sleep_data = pd.DataFrame(columns=['user_id', 'activity_hour', 'sleep_minutes'])\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    # First, merge steps and calories\n",
    "    if not steps_data.empty and not calories_data.empty:\n",
    "        merged_data = pd.merge(\n",
    "            steps_data, \n",
    "            calories_data, \n",
    "            on=['user_id', 'activity_hour'],\n",
    "            how='outer'\n",
    "        )\n",
    "    elif not steps_data.empty:\n",
    "        merged_data = steps_data\n",
    "    elif not calories_data.empty:\n",
    "        merged_data = calories_data\n",
    "    else:\n",
    "        merged_data = pd.DataFrame(columns=['user_id', 'activity_hour', 'steps', 'calories'])\n",
    "    \n",
    "    # Merge with intensity data\n",
    "    if not intensity_data.empty:\n",
    "        merged_data = pd.merge(\n",
    "            merged_data,\n",
    "            intensity_data,\n",
    "            on=['user_id', 'activity_hour'],\n",
    "            how='outer'\n",
    "        )\n",
    "    \n",
    "    # Finally, merge with sleep data\n",
    "    if not sleep_data.empty:\n",
    "        merged_data = pd.merge(\n",
    "            merged_data,\n",
    "            sleep_data,\n",
    "            on=['user_id', 'activity_hour'],\n",
    "            how='outer'\n",
    "        )\n",
    "    \n",
    "    # Add updated_at column\n",
    "    merged_data['updated_at'] = datetime.now()\n",
    "    \n",
    "    # Handle potential duplicates by using the most complete record\n",
    "    if len(merged_data) > merged_data.drop_duplicates(subset=['user_id', 'activity_hour']).shape[0]:\n",
    "        print(\"Found potential duplicate records, resolving...\")\n",
    "        \n",
    "        # Count non-null values for each record\n",
    "        merged_data['non_null_count'] = merged_data.notnull().sum(axis=1)\n",
    "        \n",
    "        # Sort by count (descending) and take the first record for each user_id/hour combination\n",
    "        merged_data = merged_data.sort_values(['user_id', 'activity_hour', 'non_null_count'], \n",
    "                                             ascending=[True, True, False])\n",
    "        merged_data = merged_data.drop_duplicates(subset=['user_id', 'activity_hour'])\n",
    "        merged_data = merged_data.drop(columns=['non_null_count'])\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_data = process_and_insert_hourly_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 70434 records into hourly_data table...\n",
      "Data insertion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Inserting {len(merged_data)} records into hourly_data table...\")\n",
    "    \n",
    "    # Use pd.to_sql with a chunksize for large datasets\n",
    "    merged_data.to_sql('hourly_data', engine, if_exists='append', index=False,\n",
    "                        method='multi', chunksize=1000)\n",
    "    \n",
    "    print(\"Data insertion completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('hourly_merged_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Minute-level data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"\"\"\n",
    "   CREATE TABLE minute_data (\n",
    "        user_id         BIGINT NOT NULL,\n",
    "        activity_minute TIMESTAMP NOT NULL,\n",
    "        steps           INTEGER,\n",
    "        intensity       INTEGER,\n",
    "        mets            NUMERIC,\n",
    "        sleep           INTEGER,\n",
    "        heart_rate      INTEGER,  -- Added heart rate column\n",
    "        updated_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        PRIMARY KEY (user_id, activity_minute),\n",
    "        FOREIGN KEY (user_id) REFERENCES user_demographics(user_id)\n",
    "    );\n",
    "    \"\"\"))\n",
    "    print(\"Table created successfully!\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1445040 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteStepsNarrow_merged.csv\n",
      "Loaded 1325580 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteStepsNarrow_merged.csv\n",
      "Loaded 1445040 rows from synthetic_data/synthetic_data/synthetic_minuteSteps.csv\n",
      "Loaded 1445040 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteIntensitiesNarrow_merged.csv\n",
      "Loaded 1325580 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteIntensitiesNarrow_merged.csv\n",
      "Loaded 1445040 rows from synthetic_data/synthetic_data/synthetic_minuteIntensities.csv\n",
      "Loaded 1445040 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteMETsNarrow_merged.csv\n",
      "Loaded 1325580 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteMETsNarrow_merged.csv\n",
      "Loaded 1445040 rows from synthetic_data/synthetic_data/synthetic_minuteMETs.csv\n",
      "Loaded 198559 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv\n",
      "Loaded 188521 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv\n",
      "Loaded 198559 rows from synthetic_data/synthetic_data/synthetic_minuteSleep.csv\n",
      "Loaded 1154681 rows from archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/heartrate_seconds_merged.csv\n",
      "Loaded 2483658 rows from archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/heartrate_seconds_merged.csv\n",
      "Loaded 84132 rows from synthetic_data/synthetic_data/synthetic_heartrate.csv\n",
      "Found potential duplicate records, resolving...\n"
     ]
    }
   ],
   "source": [
    "def process_and_insert_minute_data():\n",
    "    datasets = {\n",
    "        'steps': {\n",
    "            'files': ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteStepsNarrow_merged.csv', \n",
    "                      'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteStepsNarrow_merged.csv', \n",
    "                      'synthetic_data/synthetic_data/synthetic_minuteSteps.csv'],\n",
    "            'time_col': 'ActivityMinute',\n",
    "            'value_col': 'Steps',\n",
    "            'target_col': 'steps'\n",
    "        },\n",
    "        'intensity': {\n",
    "            'files': ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteIntensitiesNarrow_merged.csv', \n",
    "                      'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteIntensitiesNarrow_merged.csv', \n",
    "                      'synthetic_data/synthetic_data/synthetic_minuteIntensities.csv'],\n",
    "            'time_col': 'ActivityMinute',\n",
    "            'value_col': 'Intensity',\n",
    "            'target_col': 'intensity'\n",
    "        },\n",
    "        'mets': {\n",
    "            'files': ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteMETsNarrow_merged.csv', \n",
    "                      'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteMETsNarrow_merged.csv', \n",
    "                      'synthetic_data/synthetic_data/synthetic_minuteMETs.csv'],\n",
    "            'time_col': 'ActivityMinute',\n",
    "            'value_col': 'METs',\n",
    "            'target_col': 'mets'\n",
    "        },\n",
    "        'sleep': {\n",
    "            'files': ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv', \n",
    "                      'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv', \n",
    "                      'synthetic_data/synthetic_data/synthetic_minuteSleep.csv'],\n",
    "            'time_col': 'date',\n",
    "            'value_col': 'value',\n",
    "            'target_col': 'sleep'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process each dataset type\n",
    "    processed_dfs = {}\n",
    "    \n",
    "    for data_type, config in datasets.items():\n",
    "        dfs = []\n",
    "        for file_path in config['files']:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                dfs.append(df)\n",
    "                print(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File {file_path} not found. Skipping.\")\n",
    "        \n",
    "        if dfs:\n",
    "            combined = pd.concat(dfs, ignore_index=True)\n",
    "            combined['Id'] = combined['Id'].astype(int)\n",
    "            combined[config['time_col']] = pd.to_datetime(combined[config['time_col']], format='mixed')\n",
    "            \n",
    "            # Rename columns to match our schema\n",
    "            renamed = combined.rename(columns={\n",
    "                'Id': 'user_id',\n",
    "                config['time_col']: 'activity_minute',\n",
    "                config['value_col']: config['target_col']\n",
    "            })\n",
    "            \n",
    "            # Select only the columns we need\n",
    "            processed_dfs[data_type] = renamed[['user_id', 'activity_minute', config['target_col']]]\n",
    "        else:\n",
    "            print(f\"No {data_type} data found.\")\n",
    "            processed_dfs[data_type] = pd.DataFrame(columns=['user_id', 'activity_minute', config['target_col']])\n",
    "    \n",
    "    # Process heart rate data separately because it's per second and needs aggregation\n",
    "    heart_dfs = []\n",
    "    paths = ['archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/heartrate_seconds_merged.csv',\n",
    "             'archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/heartrate_seconds_merged.csv',\n",
    "             'synthetic_data/synthetic_data/synthetic_heartrate.csv']\n",
    "    for file_path in paths:  # Assuming you have 3 heart rate files\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            heart_dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found. Skipping.\")\n",
    "    \n",
    "    if heart_dfs:\n",
    "        combined_heart = pd.concat(heart_dfs, ignore_index=True)\n",
    "        combined_heart['Id'] = combined_heart['Id'].astype(int)\n",
    "        combined_heart['Time'] = pd.to_datetime(combined_heart['Time'], format='mixed')  \n",
    "        \n",
    "        # Create a minute-level timestamp by truncating seconds\n",
    "        combined_heart['activity_minute'] = combined_heart['Time'].dt.floor('min')\n",
    "        \n",
    "        # Group by user and minute, taking max heart rate\n",
    "        heart_rate_data = combined_heart.groupby(['Id', 'activity_minute'])['Value'].max().reset_index()\n",
    "        \n",
    "        # Rename columns\n",
    "        heart_rate_data = heart_rate_data.rename(columns={\n",
    "            'Id': 'user_id',\n",
    "            'Value': 'heart_rate'\n",
    "        })\n",
    "    else:\n",
    "        print(\"No heart rate data found.\")\n",
    "        heart_rate_data = pd.DataFrame(columns=['user_id', 'activity_minute', 'heart_rate'])\n",
    "    \n",
    "    # Begin merging all datasets\n",
    "    # Start with steps data if available, otherwise create empty DataFrame\n",
    "    if 'steps' in processed_dfs and not processed_dfs['steps'].empty:\n",
    "        merged_data = processed_dfs['steps']\n",
    "    else:\n",
    "        merged_data = pd.DataFrame(columns=['user_id', 'activity_minute', 'steps'])\n",
    "    \n",
    "    # Merge with each other dataset\n",
    "    for data_type in ['intensity', 'mets', 'sleep']:\n",
    "        if data_type in processed_dfs and not processed_dfs[data_type].empty:\n",
    "            merged_data = pd.merge(\n",
    "                merged_data,\n",
    "                processed_dfs[data_type],\n",
    "                on=['user_id', 'activity_minute'],\n",
    "                how='outer'\n",
    "            )\n",
    "    \n",
    "    # Merge with heart rate data\n",
    "    if not heart_rate_data.empty:\n",
    "        merged_data = pd.merge(\n",
    "            merged_data,\n",
    "            heart_rate_data,\n",
    "            on=['user_id', 'activity_minute'],\n",
    "            how='outer'\n",
    "        )\n",
    "    \n",
    "    # Add updated_at column\n",
    "    merged_data['updated_at'] = datetime.now()\n",
    "    \n",
    "    # Handle potential duplicates\n",
    "    if len(merged_data) > merged_data.drop_duplicates(subset=['user_id', 'activity_minute']).shape[0]:\n",
    "        print(\"Found potential duplicate records, resolving...\")\n",
    "        \n",
    "        # Count non-null values for each record\n",
    "        merged_data['non_null_count'] = merged_data.notnull().sum(axis=1)\n",
    "        \n",
    "        # Sort by count (descending) and take the first record for each user_id/minute combination\n",
    "        merged_data = merged_data.sort_values(\n",
    "            ['user_id', 'activity_minute', 'non_null_count'], \n",
    "            ascending=[True, True, False]\n",
    "        )\n",
    "        merged_data = merged_data.drop_duplicates(subset=['user_id', 'activity_minute'])\n",
    "        merged_data = merged_data.drop(columns=['non_null_count'])\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = process_and_insert_minute_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 4436505 records into minute_data table...\n",
      "Inserted chunk 1 (10000 rows)\n",
      "Inserted chunk 2 (10000 rows)\n",
      "Inserted chunk 3 (10000 rows)\n",
      "Inserted chunk 4 (10000 rows)\n",
      "Inserted chunk 5 (10000 rows)\n",
      "Inserted chunk 6 (10000 rows)\n",
      "Inserted chunk 7 (10000 rows)\n",
      "Inserted chunk 8 (10000 rows)\n",
      "Inserted chunk 9 (10000 rows)\n",
      "Inserted chunk 10 (10000 rows)\n",
      "Inserted chunk 11 (10000 rows)\n",
      "Inserted chunk 12 (10000 rows)\n",
      "Inserted chunk 13 (10000 rows)\n",
      "Inserted chunk 14 (10000 rows)\n",
      "Inserted chunk 15 (10000 rows)\n",
      "Inserted chunk 16 (10000 rows)\n",
      "Inserted chunk 17 (10000 rows)\n",
      "Inserted chunk 18 (10000 rows)\n",
      "Inserted chunk 19 (10000 rows)\n",
      "Inserted chunk 20 (10000 rows)\n",
      "Inserted chunk 21 (10000 rows)\n",
      "Inserted chunk 22 (10000 rows)\n",
      "Inserted chunk 23 (10000 rows)\n",
      "Inserted chunk 24 (10000 rows)\n",
      "Inserted chunk 25 (10000 rows)\n",
      "Inserted chunk 26 (10000 rows)\n",
      "Inserted chunk 27 (10000 rows)\n",
      "Inserted chunk 28 (10000 rows)\n",
      "Inserted chunk 29 (10000 rows)\n",
      "Inserted chunk 30 (10000 rows)\n",
      "Inserted chunk 31 (10000 rows)\n",
      "Inserted chunk 32 (10000 rows)\n",
      "Inserted chunk 33 (10000 rows)\n",
      "Inserted chunk 34 (10000 rows)\n",
      "Inserted chunk 35 (10000 rows)\n",
      "Inserted chunk 36 (10000 rows)\n",
      "Inserted chunk 37 (10000 rows)\n",
      "Inserted chunk 38 (10000 rows)\n",
      "Inserted chunk 39 (10000 rows)\n",
      "Inserted chunk 40 (10000 rows)\n",
      "Inserted chunk 41 (10000 rows)\n",
      "Inserted chunk 42 (10000 rows)\n",
      "Inserted chunk 43 (10000 rows)\n",
      "Inserted chunk 44 (10000 rows)\n",
      "Inserted chunk 45 (10000 rows)\n",
      "Inserted chunk 46 (10000 rows)\n",
      "Inserted chunk 47 (10000 rows)\n",
      "Inserted chunk 48 (10000 rows)\n",
      "Inserted chunk 49 (10000 rows)\n",
      "Inserted chunk 50 (10000 rows)\n",
      "Inserted chunk 51 (10000 rows)\n",
      "Inserted chunk 52 (10000 rows)\n",
      "Inserted chunk 53 (10000 rows)\n",
      "Inserted chunk 54 (10000 rows)\n",
      "Inserted chunk 55 (10000 rows)\n",
      "Inserted chunk 56 (10000 rows)\n",
      "Inserted chunk 57 (10000 rows)\n",
      "Inserted chunk 58 (10000 rows)\n",
      "Inserted chunk 59 (10000 rows)\n",
      "Inserted chunk 60 (10000 rows)\n",
      "Inserted chunk 61 (10000 rows)\n",
      "Inserted chunk 62 (10000 rows)\n",
      "Inserted chunk 63 (10000 rows)\n",
      "Inserted chunk 64 (10000 rows)\n",
      "Inserted chunk 65 (10000 rows)\n",
      "Inserted chunk 66 (10000 rows)\n",
      "Inserted chunk 67 (10000 rows)\n",
      "Inserted chunk 68 (10000 rows)\n",
      "Inserted chunk 69 (10000 rows)\n",
      "Inserted chunk 70 (10000 rows)\n",
      "Inserted chunk 71 (10000 rows)\n",
      "Inserted chunk 72 (10000 rows)\n",
      "Inserted chunk 73 (10000 rows)\n",
      "Inserted chunk 74 (10000 rows)\n",
      "Inserted chunk 75 (10000 rows)\n",
      "Inserted chunk 76 (10000 rows)\n",
      "Inserted chunk 77 (10000 rows)\n",
      "Inserted chunk 78 (10000 rows)\n",
      "Inserted chunk 79 (10000 rows)\n",
      "Inserted chunk 80 (10000 rows)\n",
      "Inserted chunk 81 (10000 rows)\n",
      "Inserted chunk 82 (10000 rows)\n",
      "Inserted chunk 83 (10000 rows)\n",
      "Inserted chunk 84 (10000 rows)\n",
      "Inserted chunk 85 (10000 rows)\n",
      "Inserted chunk 86 (10000 rows)\n",
      "Inserted chunk 87 (10000 rows)\n",
      "Inserted chunk 88 (10000 rows)\n",
      "Inserted chunk 89 (10000 rows)\n",
      "Inserted chunk 90 (10000 rows)\n",
      "Inserted chunk 91 (10000 rows)\n",
      "Inserted chunk 92 (10000 rows)\n",
      "Inserted chunk 93 (10000 rows)\n",
      "Inserted chunk 94 (10000 rows)\n",
      "Inserted chunk 95 (10000 rows)\n",
      "Inserted chunk 96 (10000 rows)\n",
      "Inserted chunk 97 (10000 rows)\n",
      "Inserted chunk 98 (10000 rows)\n",
      "Inserted chunk 99 (10000 rows)\n",
      "Inserted chunk 100 (10000 rows)\n",
      "Inserted chunk 101 (10000 rows)\n",
      "Inserted chunk 102 (10000 rows)\n",
      "Inserted chunk 103 (10000 rows)\n",
      "Inserted chunk 104 (10000 rows)\n",
      "Inserted chunk 105 (10000 rows)\n",
      "Inserted chunk 106 (10000 rows)\n",
      "Inserted chunk 107 (10000 rows)\n",
      "Inserted chunk 108 (10000 rows)\n",
      "Inserted chunk 109 (10000 rows)\n",
      "Inserted chunk 110 (10000 rows)\n",
      "Inserted chunk 111 (10000 rows)\n",
      "Inserted chunk 112 (10000 rows)\n",
      "Inserted chunk 113 (10000 rows)\n",
      "Inserted chunk 114 (10000 rows)\n",
      "Inserted chunk 115 (10000 rows)\n",
      "Inserted chunk 116 (10000 rows)\n",
      "Inserted chunk 117 (10000 rows)\n",
      "Inserted chunk 118 (10000 rows)\n",
      "Inserted chunk 119 (10000 rows)\n",
      "Inserted chunk 120 (10000 rows)\n",
      "Inserted chunk 121 (10000 rows)\n",
      "Inserted chunk 122 (10000 rows)\n",
      "Inserted chunk 123 (10000 rows)\n",
      "Inserted chunk 124 (10000 rows)\n",
      "Inserted chunk 125 (10000 rows)\n",
      "Inserted chunk 126 (10000 rows)\n",
      "Inserted chunk 127 (10000 rows)\n",
      "Inserted chunk 128 (10000 rows)\n",
      "Inserted chunk 129 (10000 rows)\n",
      "Inserted chunk 130 (10000 rows)\n",
      "Inserted chunk 131 (10000 rows)\n",
      "Inserted chunk 132 (10000 rows)\n",
      "Inserted chunk 133 (10000 rows)\n",
      "Inserted chunk 134 (10000 rows)\n",
      "Inserted chunk 135 (10000 rows)\n",
      "Inserted chunk 136 (10000 rows)\n",
      "Inserted chunk 137 (10000 rows)\n",
      "Inserted chunk 138 (10000 rows)\n",
      "Inserted chunk 139 (10000 rows)\n",
      "Inserted chunk 140 (10000 rows)\n",
      "Inserted chunk 141 (10000 rows)\n",
      "Inserted chunk 142 (10000 rows)\n",
      "Inserted chunk 143 (10000 rows)\n",
      "Inserted chunk 144 (10000 rows)\n",
      "Inserted chunk 145 (10000 rows)\n",
      "Inserted chunk 146 (10000 rows)\n",
      "Inserted chunk 147 (10000 rows)\n",
      "Inserted chunk 148 (10000 rows)\n",
      "Inserted chunk 149 (10000 rows)\n",
      "Inserted chunk 150 (10000 rows)\n",
      "Inserted chunk 151 (10000 rows)\n",
      "Inserted chunk 152 (10000 rows)\n",
      "Inserted chunk 153 (10000 rows)\n",
      "Inserted chunk 154 (10000 rows)\n",
      "Inserted chunk 155 (10000 rows)\n",
      "Inserted chunk 156 (10000 rows)\n",
      "Inserted chunk 157 (10000 rows)\n",
      "Inserted chunk 158 (10000 rows)\n",
      "Inserted chunk 159 (10000 rows)\n",
      "Inserted chunk 160 (10000 rows)\n",
      "Inserted chunk 161 (10000 rows)\n",
      "Inserted chunk 162 (10000 rows)\n",
      "Inserted chunk 163 (10000 rows)\n",
      "Inserted chunk 164 (10000 rows)\n",
      "Inserted chunk 165 (10000 rows)\n",
      "Inserted chunk 166 (10000 rows)\n",
      "Inserted chunk 167 (10000 rows)\n",
      "Inserted chunk 168 (10000 rows)\n",
      "Inserted chunk 169 (10000 rows)\n",
      "Inserted chunk 170 (10000 rows)\n",
      "Inserted chunk 171 (10000 rows)\n",
      "Inserted chunk 172 (10000 rows)\n",
      "Inserted chunk 173 (10000 rows)\n",
      "Inserted chunk 174 (10000 rows)\n",
      "Inserted chunk 175 (10000 rows)\n",
      "Inserted chunk 176 (10000 rows)\n",
      "Inserted chunk 177 (10000 rows)\n",
      "Inserted chunk 178 (10000 rows)\n",
      "Inserted chunk 179 (10000 rows)\n",
      "Inserted chunk 180 (10000 rows)\n",
      "Inserted chunk 181 (10000 rows)\n",
      "Inserted chunk 182 (10000 rows)\n",
      "Inserted chunk 183 (10000 rows)\n",
      "Inserted chunk 184 (10000 rows)\n",
      "Inserted chunk 185 (10000 rows)\n",
      "Inserted chunk 186 (10000 rows)\n",
      "Inserted chunk 187 (10000 rows)\n",
      "Inserted chunk 188 (10000 rows)\n",
      "Inserted chunk 189 (10000 rows)\n",
      "Inserted chunk 190 (10000 rows)\n",
      "Inserted chunk 191 (10000 rows)\n",
      "Inserted chunk 192 (10000 rows)\n",
      "Inserted chunk 193 (10000 rows)\n",
      "Inserted chunk 194 (10000 rows)\n",
      "Inserted chunk 195 (10000 rows)\n",
      "Inserted chunk 196 (10000 rows)\n",
      "Inserted chunk 197 (10000 rows)\n",
      "Inserted chunk 198 (10000 rows)\n",
      "Inserted chunk 199 (10000 rows)\n",
      "Inserted chunk 200 (10000 rows)\n",
      "Inserted chunk 201 (10000 rows)\n",
      "Inserted chunk 202 (10000 rows)\n",
      "Inserted chunk 203 (10000 rows)\n",
      "Inserted chunk 204 (10000 rows)\n",
      "Inserted chunk 205 (10000 rows)\n",
      "Inserted chunk 206 (10000 rows)\n",
      "Inserted chunk 207 (10000 rows)\n",
      "Inserted chunk 208 (10000 rows)\n",
      "Inserted chunk 209 (10000 rows)\n",
      "Inserted chunk 210 (10000 rows)\n",
      "Inserted chunk 211 (10000 rows)\n",
      "Inserted chunk 212 (10000 rows)\n",
      "Inserted chunk 213 (10000 rows)\n",
      "Inserted chunk 214 (10000 rows)\n",
      "Inserted chunk 215 (10000 rows)\n",
      "Inserted chunk 216 (10000 rows)\n",
      "Inserted chunk 217 (10000 rows)\n",
      "Inserted chunk 218 (10000 rows)\n",
      "Inserted chunk 219 (10000 rows)\n",
      "Inserted chunk 220 (10000 rows)\n",
      "Inserted chunk 221 (10000 rows)\n",
      "Inserted chunk 222 (10000 rows)\n",
      "Inserted chunk 223 (10000 rows)\n",
      "Inserted chunk 224 (10000 rows)\n",
      "Inserted chunk 225 (10000 rows)\n",
      "Inserted chunk 226 (10000 rows)\n",
      "Inserted chunk 227 (10000 rows)\n",
      "Inserted chunk 228 (10000 rows)\n",
      "Inserted chunk 229 (10000 rows)\n",
      "Inserted chunk 230 (10000 rows)\n",
      "Inserted chunk 231 (10000 rows)\n",
      "Inserted chunk 232 (10000 rows)\n",
      "Inserted chunk 233 (10000 rows)\n",
      "Inserted chunk 234 (10000 rows)\n",
      "Inserted chunk 235 (10000 rows)\n",
      "Inserted chunk 236 (10000 rows)\n",
      "Inserted chunk 237 (10000 rows)\n",
      "Inserted chunk 238 (10000 rows)\n",
      "Inserted chunk 239 (10000 rows)\n",
      "Inserted chunk 240 (10000 rows)\n",
      "Inserted chunk 241 (10000 rows)\n",
      "Inserted chunk 242 (10000 rows)\n",
      "Inserted chunk 243 (10000 rows)\n",
      "Inserted chunk 244 (10000 rows)\n",
      "Inserted chunk 245 (10000 rows)\n",
      "Inserted chunk 246 (10000 rows)\n",
      "Inserted chunk 247 (10000 rows)\n",
      "Inserted chunk 248 (10000 rows)\n",
      "Inserted chunk 249 (10000 rows)\n",
      "Inserted chunk 250 (10000 rows)\n",
      "Inserted chunk 251 (10000 rows)\n",
      "Inserted chunk 252 (10000 rows)\n",
      "Inserted chunk 253 (10000 rows)\n",
      "Inserted chunk 254 (10000 rows)\n",
      "Inserted chunk 255 (10000 rows)\n",
      "Inserted chunk 256 (10000 rows)\n",
      "Inserted chunk 257 (10000 rows)\n",
      "Inserted chunk 258 (10000 rows)\n",
      "Inserted chunk 259 (10000 rows)\n",
      "Inserted chunk 260 (10000 rows)\n",
      "Inserted chunk 261 (10000 rows)\n",
      "Inserted chunk 262 (10000 rows)\n",
      "Inserted chunk 263 (10000 rows)\n",
      "Inserted chunk 264 (10000 rows)\n",
      "Inserted chunk 265 (10000 rows)\n",
      "Inserted chunk 266 (10000 rows)\n",
      "Inserted chunk 267 (10000 rows)\n",
      "Inserted chunk 268 (10000 rows)\n",
      "Inserted chunk 269 (10000 rows)\n",
      "Inserted chunk 270 (10000 rows)\n",
      "Inserted chunk 271 (10000 rows)\n",
      "Inserted chunk 272 (10000 rows)\n",
      "Inserted chunk 273 (10000 rows)\n",
      "Inserted chunk 274 (10000 rows)\n",
      "Inserted chunk 275 (10000 rows)\n",
      "Inserted chunk 276 (10000 rows)\n",
      "Inserted chunk 277 (10000 rows)\n",
      "Inserted chunk 278 (10000 rows)\n",
      "Inserted chunk 279 (10000 rows)\n",
      "Inserted chunk 280 (10000 rows)\n",
      "Inserted chunk 281 (10000 rows)\n",
      "Inserted chunk 282 (10000 rows)\n",
      "Inserted chunk 283 (10000 rows)\n",
      "Inserted chunk 284 (10000 rows)\n",
      "Inserted chunk 285 (10000 rows)\n",
      "Inserted chunk 286 (10000 rows)\n",
      "Inserted chunk 287 (10000 rows)\n",
      "Inserted chunk 288 (10000 rows)\n",
      "Inserted chunk 289 (10000 rows)\n",
      "Inserted chunk 290 (10000 rows)\n",
      "Inserted chunk 291 (10000 rows)\n",
      "Inserted chunk 292 (10000 rows)\n",
      "Inserted chunk 293 (10000 rows)\n",
      "Inserted chunk 294 (10000 rows)\n",
      "Inserted chunk 295 (10000 rows)\n",
      "Inserted chunk 296 (10000 rows)\n",
      "Inserted chunk 297 (10000 rows)\n",
      "Inserted chunk 298 (10000 rows)\n",
      "Inserted chunk 299 (10000 rows)\n",
      "Inserted chunk 300 (10000 rows)\n",
      "Inserted chunk 301 (10000 rows)\n",
      "Inserted chunk 302 (10000 rows)\n",
      "Inserted chunk 303 (10000 rows)\n",
      "Inserted chunk 304 (10000 rows)\n",
      "Inserted chunk 305 (10000 rows)\n",
      "Inserted chunk 306 (10000 rows)\n",
      "Inserted chunk 307 (10000 rows)\n",
      "Inserted chunk 308 (10000 rows)\n",
      "Inserted chunk 309 (10000 rows)\n",
      "Inserted chunk 310 (10000 rows)\n",
      "Inserted chunk 311 (10000 rows)\n",
      "Inserted chunk 312 (10000 rows)\n",
      "Inserted chunk 313 (10000 rows)\n",
      "Inserted chunk 314 (10000 rows)\n",
      "Inserted chunk 315 (10000 rows)\n",
      "Inserted chunk 316 (10000 rows)\n",
      "Inserted chunk 317 (10000 rows)\n",
      "Inserted chunk 318 (10000 rows)\n",
      "Inserted chunk 319 (10000 rows)\n",
      "Inserted chunk 320 (10000 rows)\n",
      "Inserted chunk 321 (10000 rows)\n",
      "Inserted chunk 322 (10000 rows)\n",
      "Inserted chunk 323 (10000 rows)\n",
      "Inserted chunk 324 (10000 rows)\n",
      "Inserted chunk 325 (10000 rows)\n",
      "Inserted chunk 326 (10000 rows)\n",
      "Inserted chunk 327 (10000 rows)\n",
      "Inserted chunk 328 (10000 rows)\n",
      "Inserted chunk 329 (10000 rows)\n",
      "Inserted chunk 330 (10000 rows)\n",
      "Inserted chunk 331 (10000 rows)\n",
      "Inserted chunk 332 (10000 rows)\n",
      "Inserted chunk 333 (10000 rows)\n",
      "Inserted chunk 334 (10000 rows)\n",
      "Inserted chunk 335 (10000 rows)\n",
      "Inserted chunk 336 (10000 rows)\n",
      "Inserted chunk 337 (10000 rows)\n",
      "Inserted chunk 338 (10000 rows)\n",
      "Inserted chunk 339 (10000 rows)\n",
      "Inserted chunk 340 (10000 rows)\n",
      "Inserted chunk 341 (10000 rows)\n",
      "Inserted chunk 342 (10000 rows)\n",
      "Inserted chunk 343 (10000 rows)\n",
      "Inserted chunk 344 (10000 rows)\n",
      "Inserted chunk 345 (10000 rows)\n",
      "Inserted chunk 346 (10000 rows)\n",
      "Inserted chunk 347 (10000 rows)\n",
      "Inserted chunk 348 (10000 rows)\n",
      "Inserted chunk 349 (10000 rows)\n",
      "Inserted chunk 350 (10000 rows)\n",
      "Inserted chunk 351 (10000 rows)\n",
      "Inserted chunk 352 (10000 rows)\n",
      "Inserted chunk 353 (10000 rows)\n",
      "Inserted chunk 354 (10000 rows)\n",
      "Inserted chunk 355 (10000 rows)\n",
      "Inserted chunk 356 (10000 rows)\n",
      "Inserted chunk 357 (10000 rows)\n",
      "Inserted chunk 358 (10000 rows)\n",
      "Inserted chunk 359 (10000 rows)\n",
      "Inserted chunk 360 (10000 rows)\n",
      "Inserted chunk 361 (10000 rows)\n",
      "Inserted chunk 362 (10000 rows)\n",
      "Inserted chunk 363 (10000 rows)\n",
      "Inserted chunk 364 (10000 rows)\n",
      "Inserted chunk 365 (10000 rows)\n",
      "Inserted chunk 366 (10000 rows)\n",
      "Inserted chunk 367 (10000 rows)\n",
      "Inserted chunk 368 (10000 rows)\n",
      "Inserted chunk 369 (10000 rows)\n",
      "Inserted chunk 370 (10000 rows)\n",
      "Inserted chunk 371 (10000 rows)\n",
      "Inserted chunk 372 (10000 rows)\n",
      "Inserted chunk 373 (10000 rows)\n",
      "Inserted chunk 374 (10000 rows)\n",
      "Inserted chunk 375 (10000 rows)\n",
      "Inserted chunk 376 (10000 rows)\n",
      "Inserted chunk 377 (10000 rows)\n",
      "Inserted chunk 378 (10000 rows)\n",
      "Inserted chunk 379 (10000 rows)\n",
      "Inserted chunk 380 (10000 rows)\n",
      "Inserted chunk 381 (10000 rows)\n",
      "Inserted chunk 382 (10000 rows)\n",
      "Inserted chunk 383 (10000 rows)\n",
      "Inserted chunk 384 (10000 rows)\n",
      "Inserted chunk 385 (10000 rows)\n",
      "Inserted chunk 386 (10000 rows)\n",
      "Inserted chunk 387 (10000 rows)\n",
      "Inserted chunk 388 (10000 rows)\n",
      "Inserted chunk 389 (10000 rows)\n",
      "Inserted chunk 390 (10000 rows)\n",
      "Inserted chunk 391 (10000 rows)\n",
      "Inserted chunk 392 (10000 rows)\n",
      "Inserted chunk 393 (10000 rows)\n",
      "Inserted chunk 394 (10000 rows)\n",
      "Inserted chunk 395 (10000 rows)\n",
      "Inserted chunk 396 (10000 rows)\n",
      "Inserted chunk 397 (10000 rows)\n",
      "Inserted chunk 398 (10000 rows)\n",
      "Inserted chunk 399 (10000 rows)\n",
      "Inserted chunk 400 (10000 rows)\n",
      "Inserted chunk 401 (10000 rows)\n",
      "Inserted chunk 402 (10000 rows)\n",
      "Inserted chunk 403 (10000 rows)\n",
      "Inserted chunk 404 (10000 rows)\n",
      "Inserted chunk 405 (10000 rows)\n",
      "Inserted chunk 406 (10000 rows)\n",
      "Inserted chunk 407 (10000 rows)\n",
      "Inserted chunk 408 (10000 rows)\n",
      "Inserted chunk 409 (10000 rows)\n",
      "Inserted chunk 410 (10000 rows)\n",
      "Inserted chunk 411 (10000 rows)\n",
      "Inserted chunk 412 (10000 rows)\n",
      "Inserted chunk 413 (10000 rows)\n",
      "Inserted chunk 414 (10000 rows)\n",
      "Inserted chunk 415 (10000 rows)\n",
      "Inserted chunk 416 (10000 rows)\n",
      "Inserted chunk 417 (10000 rows)\n",
      "Inserted chunk 418 (10000 rows)\n",
      "Inserted chunk 419 (10000 rows)\n",
      "Inserted chunk 420 (10000 rows)\n",
      "Inserted chunk 421 (10000 rows)\n",
      "Inserted chunk 422 (10000 rows)\n",
      "Inserted chunk 423 (10000 rows)\n",
      "Inserted chunk 424 (10000 rows)\n",
      "Inserted chunk 425 (10000 rows)\n",
      "Inserted chunk 426 (10000 rows)\n",
      "Inserted chunk 427 (10000 rows)\n",
      "Inserted chunk 428 (10000 rows)\n",
      "Inserted chunk 429 (10000 rows)\n",
      "Inserted chunk 430 (10000 rows)\n",
      "Inserted chunk 431 (10000 rows)\n",
      "Inserted chunk 432 (10000 rows)\n",
      "Inserted chunk 433 (10000 rows)\n",
      "Inserted chunk 434 (10000 rows)\n",
      "Inserted chunk 435 (10000 rows)\n",
      "Inserted chunk 436 (10000 rows)\n",
      "Inserted chunk 437 (10000 rows)\n",
      "Inserted chunk 438 (10000 rows)\n",
      "Inserted chunk 439 (10000 rows)\n",
      "Inserted chunk 440 (10000 rows)\n",
      "Inserted chunk 441 (10000 rows)\n",
      "Inserted chunk 442 (10000 rows)\n",
      "Inserted chunk 443 (10000 rows)\n",
      "Inserted chunk 444 (6505 rows)\n",
      "Data insertion completed successfully.\n"
     ]
    }
   ],
   "source": [
    " # Insert data into PostgreSQL table\n",
    "try:\n",
    "    print(f\"Inserting {len(result)} records into minute_data table...\")\n",
    "    \n",
    "    # Use chunking for large datasets\n",
    "    chunk_size = 10000  # Adjust based on your system's capabilities\n",
    "    total_rows = len(result)\n",
    "    \n",
    "    for i in range(0, total_rows, chunk_size):\n",
    "        chunk = result.iloc[i:i + chunk_size]\n",
    "        chunk.to_sql('minute_data', engine, if_exists='append', index=False)\n",
    "        print(f\"Inserted chunk {i//chunk_size + 1} ({len(chunk)} rows)\")\n",
    "    \n",
    "    print(\"Data insertion completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")\n",
    "    # Consider more detailed error handling if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('minute_merged_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
